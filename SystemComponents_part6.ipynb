{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b355612c15a90d39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:11:21.124410Z",
     "start_time": "2024-11-15T17:11:20.343330Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newsapi import newsapi_client\n",
    "import os\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab508ba077aa2da",
   "metadata": {},
   "source": [
    "Data Collection & API set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfac542148d5c883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:58:54.505466Z",
     "start_time": "2024-11-15T16:58:54.499980Z"
    }
   },
   "outputs": [],
   "source": [
    "api_key = os.getenv('NEWSAPI_KEY') # 환경 변수 설정\n",
    "auth = newsapi_client.NewsApiClient(api_key='d5b0dce5cb5543b6aa800e8c60689d8c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f7b928b8b2d77b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:58:56.843424Z",
     "start_time": "2024-11-15T16:58:56.379551Z"
    }
   },
   "outputs": [],
   "source": [
    "news1 = auth.get_everything(q=\"Trump\",from_param=\"2024-11-12\",to=\"2024-11-13\",language=\"en\",sort_by=\"relevancy\",page_size=5)\n",
    "news2 = auth.get_everything(q=\"Trump\",from_param=\"2024-11-11\",to=\"2024-11-12\",language=\"en\",sort_by=\"relevancy\",page_size=5)\n",
    "news3 = auth.get_everything(q=\"Trump\",from_param=\"2024-11-10\",to=\"2024-11-11\",language=\"en\",sort_by=\"relevancy\",page_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42e58ca988246a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:59:02.636905Z",
     "start_time": "2024-11-15T16:58:58.708306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "aapl = yf.download(tickers='AAPL', start='2024-11-12',end='2024-11-13',interval='15m')\n",
    "\n",
    "msft = yf.download(tickers='MSFT', start='2024-11-12',end='2024-11-13',interval='15m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:59:06.597849Z",
     "start_time": "2024-11-15T16:59:06.589642Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLake:\n",
    "    def __init__(self,db_name=\"datalake.db\"):\n",
    "        self.raw_data = {} # raw json type\n",
    "        self.processed_data = {}\n",
    "        self.db_name = db_name\n",
    "        \n",
    "    def access_control(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            password = \"1234\"\n",
    "            input_pw = input(\"Type in your password\")\n",
    "            if input_pw == password:\n",
    "                result = func(*args,**kwargs)\n",
    "            else:\n",
    "                result = \"You have no access to this DataLake!\"\n",
    "            return result\n",
    "        return wrapper\n",
    "        \n",
    "    @access_control\n",
    "    def store_data(self, dataset_name, data, processed=False): # adding data\n",
    "        if processed:\n",
    "            with sqlite3.connect(self.db_name) as conn:\n",
    "                if dataset_name not in self.processed_data:\n",
    "                    self.processed_data[dataset_name] = data\n",
    "                    data.to_sql(dataset_name, conn, index=False, if_exists='replace')\n",
    "                else:  # Append new data if table already exists\n",
    "                    self.processed_data[dataset_name] = pd.concat([self.processed_data[dataset_name], data])\n",
    "                    data.to_sql(dataset_name, conn, index=False, if_exists='append')\n",
    "\n",
    "        else:\n",
    "            if dataset_name not in self.raw_data:\n",
    "                self.raw_data[dataset_name] = data\n",
    "            else: # preventing overrridinng\n",
    "                self.raw_data[dataset_name] += data\n",
    "    \n",
    "    @access_control\n",
    "    def retrieve_data(self, dataset_name, processed=False, sql_query = None): # for data filtering and extraction\n",
    "        if processed: # assuming value for each processed key is a dataframe\n",
    "            with sqlite3.connect(self.db_name) as conn:\n",
    "                if sql_query is None:\n",
    "                    sql_query = f\"SELECT * FROM {dataset_name}\"\n",
    "                try:\n",
    "                    query_data = pd.read_sql_query(sql_query, conn)\n",
    "                    return query_data\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    return None\n",
    "            \n",
    "        else: # if raw data just return its raw data\n",
    "            return self.raw_data[dataset_name]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c47a388c713d55d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:59:27.780912Z",
     "start_time": "2024-11-15T16:59:10.352458Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type in your password 1234\n",
      "Type in your password 1234\n",
      "Type in your password 1234\n",
      "Type in your password 1234\n",
      "Type in your password 1234\n",
      "Type in your password 1234\n"
     ]
    }
   ],
   "source": [
    "test = DataLake()\n",
    "test.store_data(\"test\",news1['articles'])\n",
    "data = test.retrieve_data(\"test\")\n",
    "test.store_data(\"test1\",news2['articles'])\n",
    "test.store_data(\"test2\",news3['articles'])\n",
    "test.store_data(\"aapl\",aapl,processed=True)\n",
    "test.store_data(\"msft\",msft,processed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8016233d2318e891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T16:59:35.986976Z",
     "start_time": "2024-11-15T16:59:32.906118Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type in your password 1234\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.914993</td>\n",
       "      <td>224.235001</td>\n",
       "      <td>223.690002</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>863846</td>\n",
       "      <td>419.540710</td>\n",
       "      <td>420.010010</td>\n",
       "      <td>418.559998</td>\n",
       "      <td>418.744995</td>\n",
       "      <td>418.744995</td>\n",
       "      <td>716553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.914993</td>\n",
       "      <td>224.235001</td>\n",
       "      <td>223.690002</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>863846</td>\n",
       "      <td>418.799988</td>\n",
       "      <td>420.260010</td>\n",
       "      <td>417.709991</td>\n",
       "      <td>420.200012</td>\n",
       "      <td>420.200012</td>\n",
       "      <td>613220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223.914993</td>\n",
       "      <td>224.235001</td>\n",
       "      <td>223.690002</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>863846</td>\n",
       "      <td>420.200012</td>\n",
       "      <td>420.299988</td>\n",
       "      <td>418.809998</td>\n",
       "      <td>418.989990</td>\n",
       "      <td>418.989990</td>\n",
       "      <td>400057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>223.914993</td>\n",
       "      <td>224.235001</td>\n",
       "      <td>223.690002</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>863846</td>\n",
       "      <td>418.980011</td>\n",
       "      <td>419.965393</td>\n",
       "      <td>418.750000</td>\n",
       "      <td>419.730011</td>\n",
       "      <td>419.730011</td>\n",
       "      <td>346740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>223.914993</td>\n",
       "      <td>224.235001</td>\n",
       "      <td>223.690002</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>224.169998</td>\n",
       "      <td>863846</td>\n",
       "      <td>419.765015</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>418.820007</td>\n",
       "      <td>419.000214</td>\n",
       "      <td>419.000214</td>\n",
       "      <td>450805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>224.820007</td>\n",
       "      <td>224.460007</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>763948</td>\n",
       "      <td>423.309998</td>\n",
       "      <td>423.399994</td>\n",
       "      <td>422.899994</td>\n",
       "      <td>423.265015</td>\n",
       "      <td>423.265015</td>\n",
       "      <td>236394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>224.820007</td>\n",
       "      <td>224.460007</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>763948</td>\n",
       "      <td>423.282501</td>\n",
       "      <td>423.839996</td>\n",
       "      <td>423.079987</td>\n",
       "      <td>423.709991</td>\n",
       "      <td>423.709991</td>\n",
       "      <td>292904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>224.820007</td>\n",
       "      <td>224.460007</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>763948</td>\n",
       "      <td>423.739990</td>\n",
       "      <td>424.439911</td>\n",
       "      <td>423.630096</td>\n",
       "      <td>424.007690</td>\n",
       "      <td>424.007690</td>\n",
       "      <td>369294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>224.820007</td>\n",
       "      <td>224.460007</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>763948</td>\n",
       "      <td>423.950012</td>\n",
       "      <td>424.049988</td>\n",
       "      <td>423.300293</td>\n",
       "      <td>423.552002</td>\n",
       "      <td>423.552002</td>\n",
       "      <td>558675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>224.820007</td>\n",
       "      <td>224.460007</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>224.600006</td>\n",
       "      <td>763948</td>\n",
       "      <td>423.489990</td>\n",
       "      <td>423.529999</td>\n",
       "      <td>423.019989</td>\n",
       "      <td>423.378693</td>\n",
       "      <td>423.378693</td>\n",
       "      <td>432044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open        High         Low       Close   Adj Close  Volume  \\\n",
       "0    223.914993  224.235001  223.690002  224.169998  224.169998  863846   \n",
       "1    223.914993  224.235001  223.690002  224.169998  224.169998  863846   \n",
       "2    223.914993  224.235001  223.690002  224.169998  224.169998  863846   \n",
       "3    223.914993  224.235001  223.690002  224.169998  224.169998  863846   \n",
       "4    223.914993  224.235001  223.690002  224.169998  224.169998  863846   \n",
       "..          ...         ...         ...         ...         ...     ...   \n",
       "355  224.500000  224.820007  224.460007  224.600006  224.600006  763948   \n",
       "356  224.500000  224.820007  224.460007  224.600006  224.600006  763948   \n",
       "357  224.500000  224.820007  224.460007  224.600006  224.600006  763948   \n",
       "358  224.500000  224.820007  224.460007  224.600006  224.600006  763948   \n",
       "359  224.500000  224.820007  224.460007  224.600006  224.600006  763948   \n",
       "\n",
       "           Open        High         Low       Close   Adj Close  Volume  \n",
       "0    419.540710  420.010010  418.559998  418.744995  418.744995  716553  \n",
       "1    418.799988  420.260010  417.709991  420.200012  420.200012  613220  \n",
       "2    420.200012  420.299988  418.809998  418.989990  418.989990  400057  \n",
       "3    418.980011  419.965393  418.750000  419.730011  419.730011  346740  \n",
       "4    419.765015  420.000000  418.820007  419.000214  419.000214  450805  \n",
       "..          ...         ...         ...         ...         ...     ...  \n",
       "355  423.309998  423.399994  422.899994  423.265015  423.265015  236394  \n",
       "356  423.282501  423.839996  423.079987  423.709991  423.709991  292904  \n",
       "357  423.739990  424.439911  423.630096  424.007690  424.007690  369294  \n",
       "358  423.950012  424.049988  423.300293  423.552002  423.552002  558675  \n",
       "359  423.489990  423.529999  423.019989  423.378693  423.378693  432044  \n",
       "\n",
       "[360 rows x 12 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.retrieve_data(\"aapl\",processed=True,sql_query=\"SELECT * FROM aapl,msft WHERE aapl.Volume<1000000 AND msft.Volume<1000000 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65aacac462f31af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:46:39.931193Z",
     "start_time": "2024-11-15T15:46:39.921819Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataCategory:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.datasets = {}  # Dictionary to store datasets with their metadata \n",
    "\n",
    "    def add_dataset(self, dataset_name, metadata=None):\n",
    "        \"\"\"\n",
    "        param metadata: Metadata dictionary (e.g., description, parameters, etc.).\n",
    "        \"\"\"\n",
    "        if dataset_name not in self.datasets:\n",
    "            self.datasets[dataset_name] = metadata or {}\n",
    "\n",
    "    def search(self, keyword):\n",
    "        \"\"\"\n",
    "        Search for datasets within the category by keyword.\n",
    "        :param keyword: Keyword to search in dataset names or metadata.\n",
    "        :return: List of matching datasets.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for dataset_name, metadata in self.datasets.items():\n",
    "            if keyword.lower() in dataset_name.lower() or any(\n",
    "                keyword.lower() in str(value).lower() for value in metadata.values()\n",
    "            ):\n",
    "                results.append((dataset_name, metadata))\n",
    "        return results\n",
    "\n",
    "\n",
    "class DataCatalog:\n",
    "    \"\"\"\n",
    "    Organizes datasets into categories and provides metadata for easy discovery.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_lake):\n",
    "        \"\"\"\n",
    "        Initialize the catalog with a reference to the DataLake.\n",
    "        :param data_lake: An instance of the DataLake class.\n",
    "        \"\"\"\n",
    "        self.categories = {}\n",
    "        self.data_lake = data_lake  # Link to the DataLake instance\n",
    "\n",
    "    def add_category(self, category_name):\n",
    "        \"\"\"\n",
    "        Add a new category to the catalog.\n",
    "        :param category_name: Name of the category.\n",
    "        \"\"\"\n",
    "        if category_name not in self.categories:\n",
    "            self.categories[category_name] = DataCategory(category_name)\n",
    "\n",
    "    def add_dataset(self, category_name, dataset_name, data, metadata=None, processed=False):\n",
    "\n",
    "        if category_name not in self.categories:\n",
    "            self.add_category(category_name)\n",
    "\n",
    "        # Add dataset to the category\n",
    "        self.categories[category_name].add_dataset(dataset_name, metadata)\n",
    "\n",
    "        # Store the dataset in the DataLake\n",
    "        self.data_lake.store_data(dataset_name, data, processed=processed)\n",
    "\n",
    "    def list_datasets(self, category_name): # 카테고리 내 모든 table\n",
    "\n",
    "        if category_name in self.categories:\n",
    "            return [\n",
    "                {\"dataset_name\": name, \"metadata\": metadata}\n",
    "                for name, metadata in self.categories[category_name].datasets.items()\n",
    "            ]\n",
    "        return f\"Category '{category_name}' not found.\"\n",
    "\n",
    "    def search_data(self, keyword):\n",
    "\n",
    "        results = []\n",
    "        for category_name, category in self.categories.items():\n",
    "            matches = category.search(keyword) # table명 match\n",
    "            for dataset_name, metadata in matches:\n",
    "                results.append({\n",
    "                    \"category\": category_name,\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"metadata\": metadata\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def retrieve_dataset(self, dataset_name, processed=False, sql_query=None):\n",
    "        \n",
    "        return self.data_lake.retrieve_data(dataset_name, processed=processed, sql_query=sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16763504f27934f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:46:40.500338Z",
     "start_time": "2024-11-15T15:46:40.492616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataCatalog at 0x13b031d90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_catalog = DataCatalog(test)\n",
    "data_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75f3bbc31e1f5ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:48:43.875841Z",
     "start_time": "2024-11-15T15:48:37.195228Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type in your password 1234\n",
      "Type in your password 1234\n"
     ]
    }
   ],
   "source": [
    "data_catalog.add_dataset(\n",
    "    \"Equities\", \"aapl\", aapl,\n",
    "    metadata={\"description\": \"Apple stock data\", \"parameters\": [\"Adj Close\", \"Close\",\"High\",\"Low\",\"Open\",\"Volume\"]},\n",
    "    processed=True\n",
    ")\n",
    "data_catalog.add_dataset(\n",
    "    \"Equities\", \"msft\", msft,\n",
    "    metadata={\"description\": \"Microsoft stock data\", \"parameters\": [\"Adj Close\", \"Close\",\"High\",\"Low\",\"Open\",\"Volume\"]},\n",
    "    processed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "337bd0def67f9fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T15:48:50.536474Z",
     "start_time": "2024-11-15T15:48:45.239640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in 'Equities': [{'dataset_name': 'aapl', 'metadata': {'description': 'Apple stock data', 'parameters': ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']}}, {'dataset_name': 'msft', 'metadata': {'description': 'Microsoft stock data', 'parameters': ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']}}]\n",
      "Search results for 'Apple': [{'category': 'Equities', 'dataset_name': 'aapl', 'metadata': {'description': 'Apple stock data', 'parameters': ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']}}]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type in your password 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "           Open        High         Low       Close   Adj Close  Volume\n",
      "0   223.914993  224.235001  223.690002  224.169998  224.169998  863846\n",
      "1   224.999893  225.199997  224.830002  224.919998  224.919998  802025\n",
      "2   224.794998  224.839996  223.884995  223.919998  223.919998  896875\n",
      "3   223.919998  224.399994  223.860001  224.130005  224.130005  771170\n",
      "4   224.139999  224.149994  223.386002  223.419907  223.419907  712087\n",
      "5   223.419998  223.897507  223.354996  223.863007  223.863007  641269\n",
      "6   223.860001  224.470001  223.779999  224.470001  224.470001  689858\n",
      "7   224.449997  224.589996  224.285004  224.440002  224.440002  640233\n",
      "8   224.445007  224.539993  224.139999  224.149796  224.149796  650355\n",
      "9   224.139999  224.669998  224.139999  224.619995  224.619995  738805\n",
      "10  224.619995  224.759903  224.419998  224.619904  224.619904  529620\n",
      "11  224.630005  224.649994  224.460007  224.550995  224.550995  427092\n",
      "12  224.570007  224.729904  224.520096  224.529999  224.529999  485487\n",
      "13  224.529999  224.669998  224.440399  224.514999  224.514999  540670\n",
      "14  224.500000  224.820007  224.460007  224.600006  224.600006  763948\n",
      "15  223.914993  224.235001  223.690002  224.169998  224.169998  863846\n",
      "16  224.999893  225.199997  224.830002  224.919998  224.919998  802025\n",
      "17  224.794998  224.839996  223.884995  223.919998  223.919998  896875\n",
      "18  223.919998  224.399994  223.860001  224.130005  224.130005  771170\n",
      "19  224.139999  224.149994  223.386002  223.419907  223.419907  712087\n",
      "20  223.419998  223.897507  223.354996  223.863007  223.863007  641269\n",
      "21  223.860001  224.470001  223.779999  224.470001  224.470001  689858\n",
      "22  224.449997  224.589996  224.285004  224.440002  224.440002  640233\n",
      "23  224.445007  224.539993  224.139999  224.149796  224.149796  650355\n",
      "24  224.139999  224.669998  224.139999  224.619995  224.619995  738805\n",
      "25  224.619995  224.759903  224.419998  224.619904  224.619904  529620\n",
      "26  224.630005  224.649994  224.460007  224.550995  224.550995  427092\n",
      "27  224.570007  224.729904  224.520096  224.529999  224.529999  485487\n",
      "28  224.529999  224.669998  224.440399  224.514999  224.514999  540670\n",
      "29  224.500000  224.820007  224.460007  224.600006  224.600006  763948\n"
     ]
    }
   ],
   "source": [
    "# List datasets in a category\n",
    "print(\"Datasets in 'Equities':\", data_catalog.list_datasets(\"Equities\"))\n",
    "\n",
    "# Search for datasets\n",
    "print(\"Search results for 'Apple':\", data_catalog.search_data(\"Apple\"))\n",
    "\n",
    "# Retrieve a dataset with SQL query\n",
    "query_result = data_catalog.retrieve_dataset(\"AAPL\", processed=True, sql_query=\"SELECT * FROM AAPL WHERE Volume < 1000000\")\n",
    "print(\"Query result:\\n\", query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d0123-4da1-4d6d-976a-80813871e41a",
   "metadata": {},
   "source": [
    "\n",
    "## Framework Components\n",
    "\n",
    "### BaseDataModel\n",
    "\n",
    "\n",
    "`BaseDataModel` is an abstract class that provides common attributes and methods to be inherited by more specialized data models. It includes:\n",
    "\n",
    "- **Attributes:**\n",
    "  - `timestamp`: Records the time associated with the data. If not provided or invalid, defaults to the current time.\n",
    "  - `symbol`: An optional identifier, such as a stock ticker symbol.\n",
    "\n",
    "- **Methods:**\n",
    "  - `is_recent`: Checks if the `timestamp` is within a specified number of recent days.\n",
    "  - `is_above_threshold`: Determines if a given value exceeds a specified threshold.\n",
    "\n",
    "### DataWorkbench\n",
    "\n",
    "\n",
    "`DataWorkbench` serves as a centralized repository for managing multiple datasets and their associated metadata. Its primary responsibilities include:\n",
    "\n",
    "- **Data Storage:** Maintains datasets in a dictionary (`data_storage`) keyed by unique dataset names.\n",
    "- **Metadata Storage:** Keeps metadata related to each dataset in a separate dictionary (`metadata_storage`).\n",
    "- **Data Operations:**\n",
    "  - **Storing Data:** Adds new datasets along with optional metadata.\n",
    "  - **Retrieving Data:** Fetches datasets and their metadata by name.\n",
    "  - **Transforming Data:** Applies transformation functions to datasets and stores the results.\n",
    "  - **Aggregating Data:** Performs group-by aggregations based on specified columns and functions.\n",
    "  - **Statistical Analysis:** Computes basic statistics (e.g., mean, standard deviation) for datasets.\n",
    "\n",
    "### IntradayDataModel\n",
    "\n",
    "`IntradayDataModel` is tailored for handling intraday stock data, providing specialized methods to process and analyze such data effectively. Key functionalities include:\n",
    "\n",
    "- **Aggregation by Interval:** Resamples stock data into specified time intervals (e.g., hourly) to compute aggregated statistics like average closing price and total volume.\n",
    "- **VWAP Calculation:** Computes the Volume Weighted Average Price, offering a measure that accounts for both price and trading volume.\n",
    "- **Moving Average Calculation:** Determines the moving average of closing prices over a defined window, useful for trend analysis.\n",
    "\n",
    "### NewsDataModel\n",
    "\n",
    "`NewsDataModel` is specialized for handling news articles data, offering functionalities that bridge news sentiment with temporal analysis. Its main features include:\n",
    "\n",
    "- **Sentiment Filtering:** Selects news articles that meet or exceed a specified sentiment score, aiding in focusing on more impactful news.\n",
    "- **Grouping by Date:** Aggregates news articles by their publication dates, calculating both the count of articles and the average sentiment score for each date.\n",
    "- **Sentiment Trend Analysis:** Examines how sentiment scores evolve over time, providing insights into trends and shifts in public sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "## Utility Functions\n",
    "\n",
    "\n",
    "The `display_dataframe` function is a simple utility designed to print the name of a DataFrame followed by its first few rows. This enhances readability when outputting multiple DataFrames by providing clear section headers.\n",
    "\n",
    "---\n",
    "\n",
    "## Mock Data Creation\n",
    "\n",
    "Given that real-time data fetching (especially for future dates) isn't feasible in this context, mock data is generated to simulate realistic scenarios for both news articles and stock data.\n",
    "\n",
    "### Mock NewsAPI Responses\n",
    "\n",
    "\n",
    "Three responses (`news1`, `news2`, `news3`) contain a list of articles with attributes such as:\n",
    "\n",
    "- `publishedAt`: The publication timestamp.\n",
    "- `title`: The headline of the article.\n",
    "- `description`: A brief summary.\n",
    "- `url`: A link to the full article.\n",
    "\n",
    "### Extracting News Data\n",
    "\n",
    "The above list comprehension iterates over each mock news response and its articles, extracting relevant fields to construct a consolidated pandas DataFrame (`news_data`). For simplicity, fixed values are assigned to `sentiment_score` and `relevance`.\n",
    "\n",
    "### Mock Stock Data\n",
    "\n",
    "\n",
    "Two pandas DataFrames (`aapl_data` and `msft_data`) are created to simulate stock data for Apple Inc. (AAPL) and Microsoft Corporation (MSFT). Each DataFrame includes:\n",
    "\n",
    "- `Open`: Opening price for the interval.\n",
    "- `High`: Highest price within the interval.\n",
    "- `Low`: Lowest price within the interval.\n",
    "- `Close`: Closing price for the interval.\n",
    "- `Volume`: Trading volume within the interval.\n",
    "- `timestamp`: Date and time of the data point, generated at 15-minute intervals starting from November 12, 2024, at 09:30 AM.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Processing Workflow\n",
    "\n",
    "### Storing Data\n",
    "\n",
    "An instance of `DataWorkbench` is created to manage the datasets. The mock news and stock data are stored in the workbench with unique identifiers:\n",
    "\n",
    "- `\"news_data\"`: Contains the consolidated news articles.\n",
    "- `\"aapl_data\"`: Holds Apple stock data.\n",
    "- `\"msft_data\"`: Contains Microsoft stock data.\n",
    "\n",
    "### Aggregating Stock Data\n",
    "\n",
    "\n",
    "An instance of `IntradayDataModel` is created for AAPL with the current timestamp. The `aggregate_by_interval` method is then invoked to resample the AAPL stock data into 1-hour intervals, computing the average closing price and total volume for each hour.\n",
    "\n",
    "### Calculating VWAP\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "The `calculate_vwap` method computes the Volume Weighted Average Price (VWAP) for the entire AAPL dataset. VWAP provides a measure of the average price at which a stock has traded throughout the day, weighted by trading volume.\n",
    "\n",
    "### Analyzing News Data\n",
    "\n",
    "\n",
    "\n",
    "An instance of `NewsDataModel` is created, after which the following analyses are performed:\n",
    "\n",
    "1. **Filtering:** Selects news articles with a `sentiment_score` of 0.4 or higher.\n",
    "2. **Grouping:** Aggregates articles by their publication dates, calculating the number of articles and the average sentiment score for each date.\n",
    "3. **Trend Analysis:** Examines how the average sentiment score changes over different dates, identifying trends in public sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "## Displaying Results\n",
    "\n",
    "\n",
    "The `display_dataframe` utility function is employed to print the names and the initial rows of the processed DataFrames. Additionally, the VWAP value for AAPL is printed separately for clarity. The outputs include:\n",
    "\n",
    "- **Aggregated AAPL Data:** Resampled stock data at 1-hour intervals.\n",
    "- **VWAP for AAPL:** The Volume Weighted Average Price.\n",
    "- **Filtered News Data:** News articles meeting the sentiment threshold.\n",
    "- **Grouped News by Date:** Aggregated article counts and average sentiment scores per date.\n",
    "- **Sentiment Trend:** Evolution of average sentiment scores over time.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Output\n",
    "\n",
    "\n",
    "**Explanation of Output:**\n",
    "\n",
    "1. **Aggregated AAPL Data:**\n",
    "   - Shows the average closing price and total trading volume for each 1-hour interval.\n",
    "   \n",
    "2. **VWAP for AAPL:**\n",
    "   - Displays the Volume Weighted Average Price, calculated as 155.0 based on the mock data.\n",
    "   \n",
    "3. **Filtered News Data:**\n",
    "   - Lists news articles that have a sentiment score of 0.4 or higher.\n",
    "   \n",
    "4. **Grouped News by Date:**\n",
    "   - Provides the count of articles and average sentiment score for each publication date.\n",
    "   \n",
    "5. **Sentiment Trend:**\n",
    "   - Illustrates the stability of sentiment scores over the analyzed dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0ecfc5b-e7e1-44dd-a3a8-8e154ee99013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "# BaseDataModel for shared attributes and methods\n",
    "class BaseDataModel:\n",
    "    \"\"\"\n",
    "    BaseDataModel serves as the foundational class for data models,\n",
    "    providing shared attributes and utility methods that can be\n",
    "    inherited by more specialized data models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timestamp, symbol=None):\n",
    "        \"\"\"\n",
    "        Initializes the BaseDataModel with a timestamp and an optional symbol.\n",
    "\n",
    "        :param timestamp: The timestamp associated with the data model.\n",
    "                          If not a datetime object, defaults to the current datetime.\n",
    "        :param symbol: (Optional) A symbol identifier (e.g., stock ticker).\n",
    "        \"\"\"\n",
    "        # Check if 'timestamp' is a datetime object; if not, assign current datetime\n",
    "        self.timestamp = timestamp if isinstance(timestamp, datetime) else datetime.now()\n",
    "        # Assign the symbol if provided; else, default to None\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def is_recent(self, days=7):\n",
    "        \"\"\"\n",
    "        Determines if the timestamp is within a recent range specified by 'days'.\n",
    "\n",
    "        :param days: The number of days to consider as the \"recent\" threshold.\n",
    "        :return: Boolean indicating if the timestamp is within the recent range.\n",
    "        \"\"\"\n",
    "        # Calculate the time difference between now and the timestamp\n",
    "        delta = datetime.now() - self.timestamp\n",
    "        # Return True if the difference is less than or equal to 'days'\n",
    "        return delta.days <= days\n",
    "\n",
    "    def is_above_threshold(self, value, threshold):\n",
    "        \"\"\"\n",
    "        Checks if a given value exceeds a specified threshold.\n",
    "\n",
    "        :param value: The value to be compared against the threshold.\n",
    "        :param threshold: The threshold value to compare with.\n",
    "        :return: Boolean indicating if 'value' is greater than 'threshold'.\n",
    "        \"\"\"\n",
    "        return value > threshold\n",
    "\n",
    "# Optimized DataWorkbench\n",
    "class DataWorkbench:\n",
    "    \"\"\"\n",
    "    DataWorkbench acts as a centralized storage and processing hub for various datasets.\n",
    "    It allows storing, retrieving, transforming, and aggregating data, as well as accessing metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataWorkbench with empty dictionaries for data and metadata storage.\n",
    "        \"\"\"\n",
    "        # Dictionary to store datasets by their names\n",
    "        self.data_storage = {}\n",
    "        # Dictionary to store metadata associated with each dataset\n",
    "        self.metadata_storage = {}\n",
    "\n",
    "    def store_data(self, dataset_name, data, metadata=None):\n",
    "        \"\"\"\n",
    "        Stores a dataset in the workbench with an optional metadata dictionary.\n",
    "\n",
    "        :param dataset_name: A unique identifier for the dataset.\n",
    "        :param data: The actual dataset to be stored (e.g., pandas DataFrame).\n",
    "        :param metadata: (Optional) Additional information or attributes related to the dataset.\n",
    "        \"\"\"\n",
    "        self.data_storage[dataset_name] = data\n",
    "        self.metadata_storage[dataset_name] = metadata or {}\n",
    "\n",
    "    def retrieve_data(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieves a dataset by its name from the workbench.\n",
    "\n",
    "        :param dataset_name: The unique identifier of the dataset to retrieve.\n",
    "        :return: The dataset if found; otherwise, None.\n",
    "        \"\"\"\n",
    "        return self.data_storage.get(dataset_name, None)\n",
    "\n",
    "    def get_metadata(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieves the metadata associated with a specific dataset.\n",
    "\n",
    "        :param dataset_name: The unique identifier of the dataset.\n",
    "        :return: Metadata dictionary if found; otherwise, an empty dictionary.\n",
    "        \"\"\"\n",
    "        return self.metadata_storage.get(dataset_name, {})\n",
    "\n",
    "    def transform_data(self, dataset_name, transformation_func):\n",
    "        \"\"\"\n",
    "        Applies a transformation function to a specified dataset and stores the transformed data.\n",
    "\n",
    "        :param dataset_name: The name of the dataset to transform.\n",
    "        :param transformation_func: A function that takes a dataset as input and returns the transformed dataset.\n",
    "        :return: The transformed dataset.\n",
    "        :raises ValueError: If the specified dataset is not found.\n",
    "        \"\"\"\n",
    "        # Retrieve the dataset using its name\n",
    "        data = self.retrieve_data(dataset_name)\n",
    "        if data is not None:\n",
    "            # Apply the transformation function to the dataset\n",
    "            transformed_data = transformation_func(data)\n",
    "            # Store the transformed data with a new name indicating transformation\n",
    "            self.store_data(f\"{dataset_name}_transformed\", transformed_data)\n",
    "            return transformed_data\n",
    "        # Raise an error if the dataset does not exist in storage\n",
    "        raise ValueError(f\"Dataset {dataset_name} not found\")\n",
    "\n",
    "    def aggregate_data(self, dataset_name, group_by_column, agg_funcs):\n",
    "        \"\"\"\n",
    "        Aggregates data in a dataset by a specified column using provided aggregation functions.\n",
    "\n",
    "        :param dataset_name: The name of the dataset to aggregate.\n",
    "        :param group_by_column: The column name to group the data by.\n",
    "        :param agg_funcs: A dictionary specifying the aggregation functions for each column.\n",
    "                          Example: {'Close': 'mean', 'Volume': 'sum'}\n",
    "        :return: The aggregated dataset as a pandas DataFrame.\n",
    "        :raises ValueError: If the specified dataset is not found.\n",
    "        \"\"\"\n",
    "        # Retrieve the dataset using its name\n",
    "        data = self.retrieve_data(dataset_name)\n",
    "        if data is not None:\n",
    "            # Perform groupby aggregation based on the provided parameters\n",
    "            aggregated = data.groupby(group_by_column).agg(agg_funcs)\n",
    "            # Store the aggregated data with a new name indicating aggregation\n",
    "            self.store_data(f\"{dataset_name}_aggregated\", aggregated)\n",
    "            return aggregated\n",
    "        # Raise an error if the dataset does not exist in storage\n",
    "        raise ValueError(f\"Dataset {dataset_name} not found\")\n",
    "\n",
    "    def get_statistics(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Computes basic statistical measures for a specified dataset.\n",
    "\n",
    "        :param dataset_name: The name of the dataset to analyze.\n",
    "        :return: A pandas Series containing statistical measures like mean, std, min, max, etc.\n",
    "        :raises ValueError: If the specified dataset is not found.\n",
    "        \"\"\"\n",
    "        # Retrieve the dataset using its name\n",
    "        data = self.retrieve_data(dataset_name)\n",
    "        if data is not None:\n",
    "            # Use pandas' describe method to compute statistics\n",
    "            return data.describe()\n",
    "        # Raise an error if the dataset does not exist in storage\n",
    "        raise ValueError(f\"Dataset {dataset_name} not found\")\n",
    "\n",
    "# Optimized Quant Data Models\n",
    "class IntradayDataModel(BaseDataModel):\n",
    "    \"\"\"\n",
    "    IntradayDataModel extends BaseDataModel to handle intraday stock data.\n",
    "    It provides methods for aggregating data by time intervals,\n",
    "    calculating Volume Weighted Average Price (VWAP), and computing moving averages.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timestamp, price, volume, symbol):\n",
    "        \"\"\"\n",
    "        Initializes the IntradayDataModel with additional attributes for price and volume.\n",
    "\n",
    "        :param timestamp: The timestamp associated with the data.\n",
    "        :param price: The price information (can be detailed per data point).\n",
    "        :param volume: The volume information (can be detailed per data point).\n",
    "        :param symbol: The stock symbol (e.g., 'AAPL').\n",
    "        \"\"\"\n",
    "        # Initialize the base attributes using the superclass constructor\n",
    "        super().__init__(timestamp, symbol)\n",
    "        # Assign price and volume; can be None initially\n",
    "        self.price = price\n",
    "        self.volume = volume\n",
    "\n",
    "    def aggregate_by_interval(self, data, interval):\n",
    "        \"\"\"\n",
    "        Aggregates intraday stock data by specified time intervals.\n",
    "\n",
    "        :param data: A pandas DataFrame containing intraday stock data.\n",
    "                     Must include a 'timestamp' column with datetime information.\n",
    "        :param interval: The time interval for aggregation (e.g., '5T' for 5 minutes, '1H' for 1 hour).\n",
    "        :return: A pandas DataFrame with aggregated 'Close' prices and 'Volume'.\n",
    "        :raises ValueError: If the 'timestamp' column is missing or cannot be converted to datetime.\n",
    "        \"\"\"\n",
    "        # Check if 'timestamp' column exists in the DataFrame\n",
    "        if 'timestamp' not in data.columns:\n",
    "            raise ValueError(\"The 'timestamp' column is missing from the dataset.\")\n",
    "        try:\n",
    "            # Convert 'timestamp' column to datetime objects\n",
    "            data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "        except Exception as e:\n",
    "            # Raise an error if conversion fails\n",
    "            raise ValueError(f\"Error in converting 'timestamp' to datetime: {e}\")\n",
    "\n",
    "        # Set 'timestamp' as the DataFrame index to enable resampling\n",
    "        data.set_index('timestamp', inplace=True)\n",
    "        # Resample the data based on the specified interval and aggregate\n",
    "        aggregated = data.resample(interval.lower()).agg({\n",
    "            'Close': 'mean',   # Compute the average closing price within the interval\n",
    "            'Volume': 'sum'    # Compute the total volume within the interval\n",
    "        })\n",
    "        # Reset the index to convert 'timestamp' back to a column\n",
    "        return aggregated.reset_index()\n",
    "\n",
    "    def calculate_vwap(self, data):\n",
    "        \"\"\"\n",
    "        Calculates the Volume Weighted Average Price (VWAP) for the given data.\n",
    "\n",
    "        :param data: A pandas DataFrame containing 'Close' and 'Volume' columns.\n",
    "        :return: The VWAP value as a float.\n",
    "        \"\"\"\n",
    "        # Compute the numerator as the sum of (Close price * Volume) for all data points\n",
    "        numerator = (data['Close'] * data['Volume']).sum()\n",
    "        # Compute the denominator as the total Volume\n",
    "        denominator = data['Volume'].sum()\n",
    "        # Calculate VWAP; handle division by zero if necessary\n",
    "        vwap = numerator / denominator if denominator != 0 else 0\n",
    "        return vwap\n",
    "\n",
    "    def calculate_moving_average(self, data, window):\n",
    "        \"\"\"\n",
    "        Calculates the moving average of the 'Close' price over a specified window.\n",
    "\n",
    "        :param data: A pandas DataFrame containing a 'Close' column.\n",
    "        :param window: The size of the rolling window (e.g., 5 for a 5-period moving average).\n",
    "        :return: The DataFrame with an additional 'Moving_Average' column.\n",
    "        \"\"\"\n",
    "        # Compute the rolling mean (moving average) for the 'Close' column\n",
    "        data['Moving_Average'] = data['Close'].rolling(window=window).mean()\n",
    "        return data\n",
    "\n",
    "class NewsDataModel(BaseDataModel):\n",
    "    \"\"\"\n",
    "    NewsDataModel extends BaseDataModel to handle news articles data.\n",
    "    It provides methods for filtering by sentiment, grouping by date,\n",
    "    and analyzing sentiment trends over time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, timestamp, headline, sentiment_score, relevance):\n",
    "        \"\"\"\n",
    "        Initializes the NewsDataModel with additional attributes for headlines,\n",
    "        sentiment scores, and relevance.\n",
    "\n",
    "        :param timestamp: The timestamp when the news article was published.\n",
    "        :param headline: The headline of the news article.\n",
    "        :param sentiment_score: The sentiment score assigned to the article.\n",
    "        :param relevance: The relevance score of the article.\n",
    "        \"\"\"\n",
    "        # Initialize the base attributes using the superclass constructor\n",
    "        super().__init__(timestamp)\n",
    "        # Assign headline, sentiment score, and relevance\n",
    "        self.headline = headline\n",
    "        self.sentiment_score = sentiment_score\n",
    "        self.relevance = relevance\n",
    "\n",
    "    def filter_by_sentiment(self, data, threshold):\n",
    "        \"\"\"\n",
    "        Filters news articles based on a minimum sentiment score threshold.\n",
    "\n",
    "        :param data: A pandas DataFrame containing news data with a 'sentiment_score' column.\n",
    "        :param threshold: The minimum sentiment score required for an article to be included.\n",
    "        :return: A pandas DataFrame containing only the articles that meet or exceed the threshold.\n",
    "        \"\"\"\n",
    "        # Apply a boolean mask to filter articles with sentiment_score >= threshold\n",
    "        filtered_data = data[data['sentiment_score'] >= threshold]\n",
    "        return filtered_data\n",
    "\n",
    "    def group_by_date(self, data):\n",
    "        \"\"\"\n",
    "        Groups news articles by their publication date, calculating the count of articles\n",
    "        and the average sentiment score for each date.\n",
    "\n",
    "        :param data: A pandas DataFrame containing news data with 'headline', 'timestamp',\n",
    "                     and 'sentiment_score' columns.\n",
    "        :return: A pandas DataFrame with 'date', 'article_count', and 'sentiment_score' columns.\n",
    "        :raises ValueError: If required columns are missing or contain null values.\n",
    "        \"\"\"\n",
    "        # Validate presence and non-nullity of 'headline' column\n",
    "        if 'headline' not in data.columns or data['headline'].isnull().any():\n",
    "            raise ValueError(\"The 'headline' column is missing or contains null values.\")\n",
    "        # Validate presence and non-nullity of 'sentiment_score' column\n",
    "        if 'sentiment_score' not in data.columns or data['sentiment_score'].isnull().any():\n",
    "            raise ValueError(\"The 'sentiment_score' column is missing or contains null values.\")\n",
    "\n",
    "        # Convert 'timestamp' to datetime and extract the date part\n",
    "        data['date'] = pd.to_datetime(data['timestamp']).dt.date\n",
    "        # Group by 'date' and aggregate the count of headlines and mean sentiment score\n",
    "        grouped = data.groupby('date').agg({\n",
    "            'headline': 'count',               # Count of articles per date\n",
    "            'sentiment_score': 'mean'          # Average sentiment score per date\n",
    "        }).rename(columns={'headline': 'article_count'})  # Rename 'headline' to 'article_count'\n",
    "\n",
    "        # Reset the index to convert 'date' back to a column\n",
    "        return grouped.reset_index()\n",
    "\n",
    "    def analyze_sentiment_trend(self, data):\n",
    "        \"\"\"\n",
    "        Analyzes the trend of sentiment scores over time by computing the average sentiment\n",
    "        score for each date.\n",
    "\n",
    "        :param data: A pandas DataFrame containing news data with 'timestamp' and 'sentiment_score' columns.\n",
    "        :return: A pandas DataFrame with 'date' and 'sentiment_score' columns representing the trend.\n",
    "        \"\"\"\n",
    "        # Convert 'timestamp' to datetime and extract the date part\n",
    "        data['date'] = pd.to_datetime(data['timestamp']).dt.date\n",
    "        # Group by 'date' and calculate the mean sentiment score for each date\n",
    "        trend = data.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "        return trend\n",
    "\n",
    "# Example display function replacement\n",
    "def display_dataframe(name, dataframe):\n",
    "    \"\"\"\n",
    "    Displays the name of the DataFrame and its first few rows.\n",
    "\n",
    "    :param name: A string representing the name or title of the DataFrame.\n",
    "    :param dataframe: The pandas DataFrame to be displayed.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(dataframe.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aa3e5455-78e4-488e-a52a-697f3ab89405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated AAPL Data:\n",
      "            timestamp  Close  Volume\n",
      "0 2024-11-12 09:00:00  151.5    2100\n",
      "1 2024-11-12 10:00:00  155.0    4650\n",
      "2 2024-11-12 11:00:00  159.0    5500\n",
      "\n",
      "VWAP for AAPL: 156.25102040816327\n",
      "\n",
      "\n",
      "Filtered News Data:\n",
      "              timestamp                         headline  sentiment_score  \\\n",
      "0  2024-11-14T10:00:00Z        AAPL releases new product              0.5   \n",
      "1  2024-11-14T12:00:00Z        MSFT acquires new company              0.5   \n",
      "2  2024-11-13T09:30:00Z     Market reacts to tech stocks              0.5   \n",
      "3  2024-11-12T15:45:00Z  Economic indicators show growth              0.5   \n",
      "4  2024-11-12T16:00:00Z         AAPL stock hits new high              0.5   \n",
      "\n",
      "   relevance  \n",
      "0        0.8  \n",
      "1        0.8  \n",
      "2        0.8  \n",
      "3        0.8  \n",
      "4        0.8  \n",
      "\n",
      "Grouped News by Date:\n",
      "         date  article_count  sentiment_score\n",
      "0  2024-11-12              2              0.5\n",
      "1  2024-11-13              1              0.5\n",
      "2  2024-11-14              2              0.5\n",
      "\n",
      "Sentiment Trend:\n",
      "         date  sentiment_score\n",
      "0  2024-11-12              0.5\n",
      "1  2024-11-13              0.5\n",
      "2  2024-11-14              0.5\n"
     ]
    }
   ],
   "source": [
    "# Test using sample `newsapi` and `yfinance` data as specified above\n",
    "# Extract relevant data from NewsAPI responses\n",
    "# This creates a pandas DataFrame with the necessary columns for processing\n",
    "news_data = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': article['publishedAt'],\n",
    "        'headline': article['title'],\n",
    "        'sentiment_score': 0.5,  # Assigning a fixed sentiment score for mock data\n",
    "        'relevance': 0.8          # Assigning a fixed relevance score for mock data\n",
    "    }\n",
    "    for news in [news1, news2, news3] for article in news['articles']\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the DataWorkbench instance\n",
    "workbench = DataWorkbench()\n",
    "\n",
    "# Store the mock datasets in the workbench with respective dataset names\n",
    "workbench.store_data(\"news_data\", news_data)   # Store news articles data\n",
    "workbench.store_data(\"aapl_data\", aapl_data)   # Store Apple stock data\n",
    "workbench.store_data(\"msft_data\", msft_data)   # Store Microsoft stock data\n",
    "\n",
    "# Instantiate the data models with current timestamps and relevant symbols\n",
    "# Note: Price and volume are set to None initially as they can be derived from the data\n",
    "intraday_model = IntradayDataModel(\n",
    "    timestamp=datetime.now(),\n",
    "    price=None,\n",
    "    volume=None,\n",
    "    symbol=\"AAPL\"  # Symbol for Apple Inc.\n",
    ")\n",
    "\n",
    "news_model = NewsDataModel(\n",
    "    timestamp=datetime.now(),\n",
    "    headline=None,\n",
    "    sentiment_score=None,\n",
    "    relevance=None\n",
    ")\n",
    "\n",
    "# Perform operations using Quant Data Models\n",
    "\n",
    "# 1. Aggregate AAPL data by 1-hour intervals\n",
    "# Use .copy() to create a copy of the DataFrame to prevent SettingWithCopyWarning\n",
    "aapl_aggregated = intraday_model.aggregate_by_interval(aapl_data.copy(), \"1H\")\n",
    "\n",
    "# 2. Calculate VWAP (Volume Weighted Average Price) for AAPL\n",
    "aapl_vwap = intraday_model.calculate_vwap(aapl_data)\n",
    "\n",
    "# 3. Analyze news data by filtering, grouping, and trend analysis\n",
    "\n",
    "# a. Filter news articles with sentiment score >= 0.4\n",
    "filtered_news = news_model.filter_by_sentiment(news_data, threshold=0.4)\n",
    "\n",
    "# b. Group news articles by their publication date, counting articles and averaging sentiment\n",
    "grouped_news = news_model.group_by_date(news_data)\n",
    "\n",
    "# c. Analyze the trend of sentiment scores over time\n",
    "sentiment_trend = news_model.analyze_sentiment_trend(news_data)\n",
    "\n",
    "# Display the processed results using the display_dataframe function\n",
    "\n",
    "# Display the aggregated AAPL stock data\n",
    "display_dataframe(\"Aggregated AAPL Data\", aapl_aggregated)\n",
    "\n",
    "# Display the calculated VWAP for AAPL\n",
    "print(f\"\\nVWAP for AAPL: {aapl_vwap}\\n\")\n",
    "\n",
    "# Display the filtered news articles based on sentiment threshold\n",
    "display_dataframe(\"Filtered News Data\", filtered_news)\n",
    "\n",
    "# Display the grouped news data by date with article counts and average sentiment\n",
    "display_dataframe(\"Grouped News by Date\", grouped_news)\n",
    "\n",
    "# Display the sentiment trend over time\n",
    "display_dataframe(\"Sentiment Trend\", sentiment_trend)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
